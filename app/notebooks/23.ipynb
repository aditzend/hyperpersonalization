{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estamos mal de recall, incluso usando el nuevo assistant api con retrieval de openai no puedo sacar toda la data. no es confiable la extraccion para una empresa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'openai' has no attribute 'Embedding'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/alexander/Projects/hyperpersonalization/app/notebooks/23.ipynb Cell 2\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alexander/Projects/hyperpersonalization/app/notebooks/23.ipynb#W1sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m load_dotenv()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alexander/Projects/hyperpersonalization/app/notebooks/23.ipynb#W1sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m OPENAI_API_TYPE \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mopenai\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/alexander/Projects/hyperpersonalization/app/notebooks/23.ipynb#W1sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m embeddings \u001b[39m=\u001b[39m OpenAIEmbeddings()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alexander/Projects/hyperpersonalization/app/notebooks/23.ipynb#W1sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m tiktoken\u001b[39m.\u001b[39mget_encoding(\u001b[39m\"\u001b[39m\u001b[39mcl100k_base\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alexander/Projects/hyperpersonalization/app/notebooks/23.ipynb#W1sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtiktoken_len\u001b[39m(text):\n",
      "File \u001b[0;32m~/Projects/hyperpersonalization/venv/lib/python3.9/site-packages/pydantic/main.py:339\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Projects/hyperpersonalization/venv/lib/python3.9/site-packages/pydantic/main.py:1050\u001b[0m, in \u001b[0;36mpydantic.main.validate_model\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Projects/hyperpersonalization/venv/lib/python3.9/site-packages/langchain/embeddings/openai.py:284\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.validate_environment\u001b[0;34m(cls, values)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    282\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mopenai\u001b[39;00m\n\u001b[0;32m--> 284\u001b[0m     values[\u001b[39m\"\u001b[39m\u001b[39mclient\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mEmbedding\n\u001b[1;32m    285\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[1;32m    286\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[1;32m    287\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCould not import openai python package. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    288\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease install it with `pip install openai`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    289\u001b[0m     )\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'openai' has no attribute 'Embedding'"
     ]
    }
   ],
   "source": [
    "\"\"\"PDF loaders\"\"\"\n",
    "import logging\n",
    "import os\n",
    "import uuid\n",
    "import json\n",
    "import requests\n",
    "import tiktoken\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import (\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
    "from langchain.document_loaders import (\n",
    "    PyPDFLoader,\n",
    "    PDFMinerPDFasHTMLLoader,\n",
    "    UnstructuredMarkdownLoader,\n",
    ")\n",
    "from dotenv import load_dotenv\n",
    "from markdownify import markdownify as md\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_TYPE = \"openai\"\n",
    "\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "\n",
    "def tiktoken_len(text):\n",
    "    \"\"\"Returns the length of a text in tokens.\"\"\"\n",
    "    tokens = tokenizer.encode(text, disallowed_special=())\n",
    "    return len(tokens)\n",
    "\n",
    "\n",
    "async def download_pdf_file_and_ingest(resource):\n",
    "    \"\"\"\n",
    "    Downloads a pdf file and ingests it into the index\n",
    "    \"\"\"\n",
    "    print(f\"[[download_pdf_file_and_ingest]] starting ...\")\n",
    "    url = resource[\"url\"]\n",
    "    index = resource[\"index\"]\n",
    "    chunk_size = 200\n",
    "    chunk_overlap = 20\n",
    "    metadata = {\"url\": url, \"index\": index, \"type\": \"pdf\"}\n",
    "    id = str(uuid.uuid1())\n",
    "\n",
    "    local_path = f\"./resources/{index}-{str(uuid.uuid1())}.pdf\"\n",
    "\n",
    "    response = requests.get(url)\n",
    "\n",
    "    with open(local_path, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    print(f\"File downloaded successfully to {local_path} !\")\n",
    "\n",
    "    try:\n",
    "        doc = PyPDFLoader(local_path).load()\n",
    "        # text_splitter = CharacterTextSplitter()\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=tiktoken_len,\n",
    "            separators=[\"\\n\\n\\n\\n\\n\", \"\\n\\n\\n\", \"\\n\\n\"],\n",
    "        )\n",
    "        chunks = text_splitter.split_documents(doc)\n",
    "        faiss_db = await load_or_create_faiss_db(\n",
    "            index_name=index, embeddings=embeddings\n",
    "        )\n",
    "        metadatas = [metadata] * len(chunks)\n",
    "        texts = []\n",
    "        for chunk in chunks:\n",
    "            # dump the metadata to string\n",
    "            metadata_text = json.dumps(metadata)\n",
    "            text = metadata_text + \"\\n\\n\" + chunk.page_content\n",
    "            texts.append(text)\n",
    "\n",
    "        faiss_db.add_texts(\n",
    "            texts=texts,\n",
    "            metadatas=metadatas,\n",
    "        )\n",
    "\n",
    "        faiss_db.save_local(f\"indexes/{index}\")\n",
    "        os.remove(local_path)\n",
    "        print(f\"Index '{index}' has a new document \")\n",
    "        return {\"status\": \"success\"}\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error while ingesting pdf {e}\")\n",
    "        return\n",
    "\n",
    "\n",
    "async def download_pdf_md_and_ingest(resource):\n",
    "    \"\"\"\n",
    "    Downloads a pdf file, converts it to markdown and ingests it into the index\n",
    "    \"\"\"\n",
    "    print(f\"[[download_pdf_file_and_ingest]] starting ...\")\n",
    "    url = resource[\"url\"]\n",
    "    index = resource[\"index\"]\n",
    "    chunk_size = 200\n",
    "    chunk_overlap = 20\n",
    "    metadata = {\"url\": url, \"index\": index, \"type\": \"pdf\"}\n",
    "    id = str(uuid.uuid1())\n",
    "\n",
    "    local_path = f\"./resources/{index}-{str(uuid.uuid1())}.pdf\"\n",
    "\n",
    "    response = requests.get(url)\n",
    "\n",
    "    with open(local_path, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    print(f\"File downloaded successfully to {local_path} !\")\n",
    "\n",
    "    try:\n",
    "        html_document = PDFMinerPDFasHTMLLoader(local_path).load()[0]\n",
    "        markdown = md(html_document.page_content)\n",
    "        # write markdown to a temporary file\n",
    "        with open(\"temp.md\", \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(markdown)\n",
    "        doc = UnstructuredMarkdownLoader(\"./temp.md\").load()\n",
    "        # text_splitter = CharacterTextSplitter()\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=tiktoken_len,\n",
    "            separators=[\"\\n\\n\\n\\n\\n\", \"\\n\\n\\n\", \"\\n\\n\"],\n",
    "        )\n",
    "        chunks = text_splitter.split_documents(doc)\n",
    "        faiss_db = await load_or_create_faiss_db(\n",
    "            index_name=index, embeddings=embeddings\n",
    "        )\n",
    "        metadatas = [metadata] * len(chunks)\n",
    "        texts = []\n",
    "        for chunk in chunks:\n",
    "            # dump the metadata to string\n",
    "            metadata_text = json.dumps(metadata)\n",
    "            text = metadata_text + \"\\n\\n\" + chunk.page_content\n",
    "            texts.append(text)\n",
    "\n",
    "        faiss_db.add_texts(\n",
    "            texts=texts,\n",
    "            metadatas=metadatas,\n",
    "        )\n",
    "\n",
    "        faiss_db.save_local(f\"indexes/{index}\")\n",
    "        os.remove(local_path)\n",
    "        print(f\"Index '{index}' has a new document \")\n",
    "        return {\"status\": \"success\"}\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error while ingesting pdf {e}\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aca pruebo una funcion nueva en la que en vez de un resumen pruebo con una lista de oraciones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import json\n",
    "\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "MODEL = \"gpt-4-1106-preview\"\n",
    "\n",
    "\n",
    "def clean_data(text: str):\n",
    "    \"\"\"Transform text using OpenAI's API.\"\"\"\n",
    "    function_description = f\"\"\"\n",
    "    The user will give you a credit card consumption report with some meaningful data surrounded by meaningless data.\n",
    "    Extract all meaningful information from the document and create a list of sentences, put those in the 'sentences' field.\n",
    "    Then create a summary of the consumptions and due payments, put that summary in the summary field.\n",
    "    Use this format:\n",
    "    'The user [did/bought/used/paid/...] [something] on [date] at [place/store] for [amount of money].'\n",
    "    Then extract the name of the user and put it in the user field.\n",
    "    Then extract the date of the document and put it in the date field.\n",
    "    No important details should be left out.\n",
    "    This is very important information for the user, so please be sure to include all the relevant information.\n",
    "    \"\"\"\n",
    "    tools = [\n",
    "        {\"type\": \"function\",\n",
    "         \"function\":{\n",
    "            \"name\": \"default\",\n",
    "            \"description\": function_description,\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"summary\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The summary of the document according to the function description.\",\n",
    "                    },\n",
    "                    \"sentences\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The sentences extracted of the document according to the function description.\",\n",
    "                    },\n",
    "                    \"user\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"[User's first name] [User's last name]\",\n",
    "                    },\n",
    "                    \"date\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Date in the format YYYY-MM-DD\",\n",
    "                    },\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "        }\n",
    "    ]\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": text},\n",
    "    ]\n",
    "    try:\n",
    "        completion = openai.ChatCompletion.create(\n",
    "            model=MODEL,\n",
    "            messages=messages,\n",
    "            tools=tools,\n",
    "            tool_choice={\"type\":\"function\", \"function\": {\"name\": \"default\"}},\n",
    "        )\n",
    "        print(completion)\n",
    "        arguments = json.loads(\n",
    "            str(completion.choices[0])\n",
    "        )\n",
    "\n",
    "        return arguments\n",
    "\n",
    "    except openai.error.InvalidRequestError:\n",
    "        print(\"OpenAI request failed\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-8IcFnieEyYLFfDkY9PAs5Dy0j9mwp\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1699447643,\n",
      "  \"model\": \"gpt-4-1106-preview\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": null,\n",
      "        \"tool_calls\": [\n",
      "          {\n",
      "            \"id\": \"call_Foh5CuSDOrXTCYwha5iAammL\",\n",
      "            \"type\": \"function\",\n",
      "            \"function\": {\n",
      "              \"name\": \"default\",\n",
      "              \"arguments\": \"{\\n  \\\"summary\\\": \\\"Alexander Ditzend used his Mastercard Platinum credit card for multiple transactions between September and October 2023. The card was charged for purchases at Scape Park, Autopistas, Don Web, Adobe, and various other vendors.\\\",\\n  \\\"sentences\\\": \\\"The user bought access to Scape Park on 24-Sep-23 for ARS 2,603.00. The user paid toll charges on 08-Sep-23 at Autopistas del 10/18. The user made payments to DON WEB on 14-Sep-23 for undisclosed amounts. The user made a purchase at PPRO *Adobe on 22-Sep-23. The user made payments on 22-Sep-23 and 01-Oct-23 at Autopistas del 10/18. The user paid for services from MERPAGO*NUEVASOCIALF on 03-Oct-23. The user purchased airline tickets from WWW.FLYBONDI.COM on 03-Oct-23. The user paid a CLARO PT bill on 04-Oct-23.\\\",\\n  \\\"user\\\": \\\"Alexander Ditzend\\\",\\n  \\\"date\\\": \\\"2023-11-09\\\"\\n}\"\n",
      "            }\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 3921,\n",
      "    \"completion_tokens\": 245,\n",
      "    \"total_tokens\": 4166\n",
      "  },\n",
      "  \"system_fingerprint\": \"fp_a24b4d720c\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "summary = clean_data(text=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': 'The user Alexander Ditzend had a total payment due of ARS 102,041.04 for his Mastercard Platinum credit card, with transactions spanning from 24-Sep-23 to 04-Oct-23, including purchases at Scape Park, toll payments, web services from Don Web, Adobe subscription, transactions at Flybondi.com and with Claro.',\n",
       " 'sentences': \"The user Alexander Ditzend paid ARS 102,041.04 on various dates for services which include: 'Scape Park' on 24-Sep-23, toll on 08-Sep-23 and 22-Sep-23, 'Don Web' services on 14-Sep-23, 'Adobe' subscription on 22-Sep-23, transaction at 'Flybondi.com' on 03-Oct-23, and 'Claro' service on 04-Oct-23.\",\n",
       " 'user': 'Alexander Ditzend',\n",
       " 'date': '2023-09-24'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2023-09-07'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary['date']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'sentences'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/Users/alexander/Projects/hyperpersonalization/app/notebooks/22.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/alexander/Projects/hyperpersonalization/app/notebooks/22.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m summary[\u001b[39m'\u001b[39;49m\u001b[39msentences\u001b[39;49m\u001b[39m'\u001b[39;49m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'sentences'"
     ]
    }
   ],
   "source": [
    "summary['sentences']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ALEXANDER DITZEND'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary['user']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paso siguiente: guardar el resumen en un indice junto con 'user' y 'date' como metadata ademas de 'source'. Que eso este disponible para hacer QA y generar preguntas de prueba con gpt4 para evaluar la capacidad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pero antes hay que tener un mejor metodo con recall completo de los movimientos, los precios y la moneda. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pruebo con otro método dentro de la función."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import json\n",
    "\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "MODEL = \"gpt-4-1106-preview\"\n",
    "\n",
    "\n",
    "def clean_data_2(text: str):\n",
    "    \"\"\"Transform text using OpenAI's API.\"\"\"\n",
    "    function_description = f\"\"\"\n",
    "    The user will give you a credit card consumption report with some meaningful data surrounded by meaningless data.\n",
    "    Extract all meaningful information from the document and create a list of sentences, put those in the 'sentences' field.\n",
    "    Then create a summary of the consumptions and due payments, put that summary in the summary field.\n",
    "    Use this format:\n",
    "    'The user [did/bought/used/paid/...] [something] on [date] at [place/store] for [amount of money].'\n",
    "    Then extract the name of the user and put it in the user field.\n",
    "    Then extract the date of the document and put it in the date field.\n",
    "    No important details should be left out.\n",
    "    This is very important information for the user, so please be sure to include all the relevant information.\n",
    "    \"\"\"\n",
    "    tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"default\",\n",
    "                \"description\": function_description,\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"summary\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The summary of the document according to the function description.\",\n",
    "                        },\n",
    "                        \"sentences\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The sentences extracted of the document according to the function description.\",\n",
    "                        },\n",
    "                        \"user\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"[User's first name] [User's last name]\",\n",
    "                        },\n",
    "                        \"date\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"Date in the format YYYY-MM-DD\",\n",
    "                        },\n",
    "                    },\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": text},\n",
    "    ]\n",
    "    try:\n",
    "        completion = openai.ChatCompletion.create(\n",
    "            model=MODEL,\n",
    "            messages=messages,\n",
    "            tools=tools,\n",
    "            tool_choice={\"type\": \"function\", \"function\": {\"name\": \"default\"}},\n",
    "        )\n",
    "        print(completion)\n",
    "        arguments = json.loads(str(completion.choices[0]))\n",
    "\n",
    "        return arguments\n",
    "\n",
    "    except openai.error.InvalidRequestError:\n",
    "        print(\"OpenAI request failed\")\n",
    "        return None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
